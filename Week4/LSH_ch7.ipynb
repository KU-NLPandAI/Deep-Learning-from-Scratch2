{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSH_ch7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWahzi1tOeCX"
      },
      "source": [
        "RNN을 이용한 문장 생성\n",
        "\n",
        "단어의 출현확률을 기반으로 1. 결정적으로 선택하는 법 2. 확률적으로 선택하는 법\n",
        "\n",
        "더 좋은 언어모델은 더 자연스러운 문장을 만들어 낸다. \n",
        "\n",
        "Seq2Seq\n",
        "\n",
        "Encoder-Decoder\n",
        "\n",
        "Decoder와 앞의 LSTM 계층과의 차이점 : LSTM은 영벡터를 hidden_state로 받지만 Decoder는 Encoder의 hidden_state를 받는다.(hidden_state가 가교역할을 한다.)\n",
        "\n",
        "패딩용 문자를 seq2seq가 처리하지 않게 하는 법 : softmax with loss계층에 마스크 기능추가\n",
        "\n",
        "전 단원에서 stateful을 True로 설정한 이유는 긴 시계열 데이터를 다뤘기 때문이었고 이번 단원에서는 짧은 시계열 데이터를 배치처리하기 때문에 은닉 상태를 초기화로 설정한다.\n",
        "\n",
        "훈련시에는 시계방향의 데이터를 한꺼번에 주지만 추론시에는 최초 시작을 알리는 문자(<eos>,'_')만을 decoder에 넘겨준다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-no1tqKw22N"
      },
      "source": [
        "seq2seq의 개선\n",
        "\n",
        "1. 입력 데이터의 반전(대응되는 문자가 바로 옆으로 이동되서 더 기울기가 잘 전해짐.)\n",
        "\n",
        "2. Peeky => LSTM계층에서 input도 concat을 진행하고 Affine계층에서의 input도 concat을 진행한다. \n",
        "\n",
        "peeky decoder를 이용하게 될 경우 매개변수가 그만큼 커지기 때문에 계산량도 늘어난다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0uJSQLA6GS6"
      },
      "source": [
        "챗봇의 seq2seq => 상대의 말을 자신의 말로 변환\n",
        "\n",
        "알고리즘의 seq2seq => 코드를 정답으로 변환\n",
        "\n",
        "이미지 캡셔닝 => 이미지를 문장으로 변환 "
      ]
    }
  ]
}