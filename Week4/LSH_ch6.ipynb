{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSH_ch6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfbFpw0w5XIz"
      },
      "source": [
        "Ch.6 게이트가 추가된 RNN\n",
        "\n",
        "RNN의 단점 : long term dependency를 잘 학습할 수 없음. => 기울기 폭발 또는 기울기 소실이 발생함.\n",
        "\n",
        "기울기 소실과 기울기 폭발의 원인 \n",
        "1. tanh의 미분 함수 그래프를 살펴보면 input(이전 노드에서의 기울기)값에 대한 모든 ouput값이 1과 0 사이의 값이라는 것을 알 수 있다. 따라서 tanh노드를 지날때마다 기울기 값은 작아질 수 있다는 것을 의미한다. \n",
        "\n",
        "[improving perfomance of recurrent neural network with relu nonlinearity]논문에서는 tanh대신 ReLU를 사용해 성능을 개선한 것을 볼 수 있습니다.\n",
        "2. 저자는 이론보다는 실험의 결과로 MatMul으로 인해 기울기가 폭발하거나 소실될 수 있다는 것을 보여줍니다. \n",
        "\n",
        "지수적인 변화의 이유 : 오차역전파가 Wh를 반복해서 곱하기 때문에 이런 현상이 일어난다. => Wh가 1보다 크면 지수적으로 증가하고 1보다 작으면 지수적으로 감소하게 된다. \n",
        "Wh가 행렬이라면 행렬의 특잇값이 척도가 되는데 특잇값은 데이터가 얼마나 퍼져있는지를 나타내게 된다. 특잇값의 최대값이 1보다 큰지 여부를 따지면 기울기 크기가 어떻게 변할지 예측할 수 있다.\n",
        "\n",
        "기울기 폭발의 대책 : 기울기 클리핑(전통적인 기법) : threshold를 넘으면 기울기를 작게 조정\n",
        "\n",
        "기울기 소실과 LSTM\n",
        "\n",
        "LSTM은 추가로 c라는 기억 셀의 추가 경로가 존재한다. \n",
        "h_t = tanh(c_t)\n",
        "\n",
        "게이트는 보통 시그모이드 함수를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZB1SE4SWMkt"
      },
      "source": [
        "output게이트\n",
        "\n",
        "o을 계산한 후 h_t값과 아다마르 곱(원소별 곱)을 수행한다. => 새로운 h_t\n",
        "\n",
        "forget게이트\n",
        "\n",
        "f를 계산한 후 c_(t-1)과 아다마르 곱 수행하고 새로운 g값을 더해준다. => c_t\n",
        "\n",
        "input게이트\n",
        "\n",
        "i의 값을 g와 아다마르 곱을 수행한다.\n",
        "\n",
        "LSTM이 기울기 소실을 없애주는 이유 : 기억 셀의 흐름만 집중해서 역전파의 흐름을 파악해보면 +연산이 일어나고 행렬곱이 아닌 아다마르 곱만 수행하기 때문에 기울기 소실의 효과를 줄여준다.\n",
        "\n",
        "affine변환 관련 행렬만 구현할 때 모아서 먼저 계산\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0gfMVRfFE0d"
      },
      "source": [
        "RNNLM의 추가 개선법\n",
        "\n",
        "1. LSTM의 계층 다층화 => 시계방향말고 상하방향으로 다층화\n",
        "\n",
        "2. dropout의 적용 => 원래는 시계 방향으로 드롭아웃 계틍을 넣는 것은 좋은 선택이 아니나 variational dropout을 적용하면서 시간 방향에 적용이 가능해짐. 같은 계층에 속하는 드롭아웃은 같은 드롭아웃 패턴을 공유함. \n",
        "\n",
        "3. 가중치 공유 => 임베딩 계층의 가중치를 어파인 계층의 가중치로 전치하여 사용함.(매개변수가 줄어드는 동시에 정확도도 향상됨.)"
      ]
    }
  ]
}