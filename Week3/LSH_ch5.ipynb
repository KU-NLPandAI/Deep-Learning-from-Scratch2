{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b000chq3STIH"
      },
      "source": [
        "지금까지는 feed forward라는 유형의 신경망(단방향)\n",
        "\n",
        "피드 포워드 신경망의 단점 : 시계열 데이터의 성질을 충분히 학습할 수 없음.\n",
        "\n",
        "해결 방법 : 순환 신경망\n",
        "\n",
        "CBOW모델의 부산물 => '단어의 의미가 인코딩된 단어의 분산표현'\n",
        "\n",
        "CBOW모델의 본래 목적 : '맥락으로부터 타깃을 추측하는 것' => 언어모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNJGmGTUZkmp"
      },
      "source": [
        "언어 모델의 정의 : 특정한 단어의 시퀀스에 대해서 그 시퀀스를 확률로 평가하는 것\n",
        "\n",
        "언어모델의 동시 확률은 확률의 곱셈 정리로부터 유도할 수 있다. => 조건부 언어모델\n",
        "\n",
        "CBOW모델을 언어모델로 사용하였을 떄의 문제점 : \n",
        "\n",
        "1. 맥락의 길이는 고정 될 수 밖에 없고 이러한 경우, 더 왼쪽에 있는 단어의 정보가 무시되기 때문에 언어모델로서 한계를 가질 수 밖에 없음.\n",
        "\n",
        "2. CBOW모델에서는 '순서'를 고려하지 않고 학습을 진행하기 떄문에('순서'를 고려하기 보다는 '분포'를 더 고려함) 맥락의 순서가 무시되므로 제대로 된 예측을 할 수 없음. => 은닉층에서 단어 벡터들이 더해져서 들어가게 되므로 맥락의 단어순서는 무시되게 됨.\n",
        "\n",
        "대체적인 방법 : Neural Probabilistic Language Model에서 Concatenation하는 방법을 제시하였지만 맥락의 크기에 비례해서 은닉층의 매개변수의 크기를 증가 시켜야됨. => 바람직하지 않음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8-Na-vkeB4A"
      },
      "source": [
        "\n",
        "RNN의 특징 : 순환하는 경로(닫힌 경로)가 있다는 것\n",
        "\n",
        "RNN의 식 h_t = tanh(h_(t-1)*W_h + x_t*W_x + b)\n",
        "\n",
        "순환구조를 펼친 후의 RNN에는 backpropagation을 적용할 수 있음. => BPTT\n",
        "\n",
        "BPTT의 문제점 : BPTT를 이용해 기울기를 구하기 위해서는 중간 데이터를 메모리에 유지되어야 한다. => 시계열 데이터가 증가함에 따라 메모리 사용량도 증가하게 된다.\n",
        "\n",
        "해결책 : Truncated BPTT(너무 길어진 신경망을 적당한 길이에서 잘라냄.)\n",
        "\n",
        "역전파가 연결되는 일련의 RNN계층을 '블록'이라고 정의한다.\n",
        "\n",
        "이슈 : truncated BPTT의 미니배치학습\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E15t_ORNlMnp"
      },
      "source": [
        "RNN계층의 구현\n",
        "\n",
        "1개의 블록에 대해 좌우의 입력과 출력을 하나로 묶으면 일련의 RNN계층을 하나의 계층으로 간주할 수 있음. => Time RNN\n",
        "\n",
        "page.212의 코드에서 self.cache를 사용하는 이유 : 역전파 계산시에 사용할 중간 데이터를 담음.(그래서 x가 튜플에 같이 담겨 있음.)\n",
        "\n",
        "Time RNN계층의 구현\n",
        "\n",
        "저자는 stateful변수를 이용하여 hidden state를 유지할 지 삭제 할지를 결정할 수 있도록 계층을 구현 했습니다.\n",
        "\n",
        "forward에서는 hidden_state가 2개로 분기하기 때문에 backpropagation시에는 dh_t + dh_next로서 합산된 기울기가 전달되어 옵니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGnTctLbfXem"
      },
      "source": [
        "Xavier 초깃값을 사용\n",
        "\n",
        "언어 모델의 평가\n",
        "\n",
        "perplexity사용 => 확률의 역수(작을 수록 좋다.)\n",
        "\n",
        "퍼플렉시티값의 해석 => '분기 수' => 선택사항의 수가 작을 수록 확실한 선택이 있다는 말이고 이는 더 좋은 언어모델이라는 것을 뜻한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4yX6YmBsxVD"
      },
      "source": [
        "정리\n",
        "\n",
        "*   RNN은 순환경로(닫힌 경로)가 있고, 'hidden state'를 기억할 수 있음.\n",
        "*   BPTT를 이용해서 순환경로를 길게 펼칠 수 있고 오파역전파를 적용할 수 있음.\n",
        "*   긴 계층을 일정한 크기로 잘라 한 개의 계층으로 모음('블록'). 블록단위로 학습을 수행함 => Truncated BPTT\n",
        "*   Truncated BPTT에서는 역전파의 연결을 끊는다.\n",
        "*   언어모델은 단어 시퀀스를 확률로 해석한다.\n",
        "*   RNN의 조건부 언어 모델은 이론상으로 이때까지 등장한 모든 단어의 정보를 기억할 수 있다.\n",
        "\n"
      ]
    }
  ]
}