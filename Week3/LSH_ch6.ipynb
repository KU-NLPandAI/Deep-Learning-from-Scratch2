{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfbFpw0w5XIz"
      },
      "source": [
        "Ch.6 게이트가 추가된 RNN\n",
        "\n",
        "RNN의 단점 : long term dependency를 잘 학습할 수 없음. => 기울기 폭발 또는 기울기 소실이 발생함.\n",
        "\n",
        "기울기 소실과 기울기 폭발의 원인 \n",
        "1. tanh의 미분 함수 그래프를 살펴보면 input(이전 노드에서의 기울기)값에 대한 모든 ouput값이 1과 0 사이의 값이라는 것을 알 수 있다. 따라서 tanh노드를 지날때마다 기울기 값은 작아질 수 있다는 것을 의미한다. \n",
        "\n",
        "[improving perfomance of recurrent neural network with relu nonlinearity]논문에서는 tanh대신 ReLU를 사용해 성능을 개선한 것을 볼 수 있습니다.\n",
        "2. 저자는 이론보다는 실험의 결과로 MatMul으로 인해 기울기가 폭발하거나 소실될 수 있다는 것을 보여줍니다. \n",
        "\n",
        "지수적인 변화의 이유 : 오차역전파가 Wh를 반복해서 곱하기 때문에 이런 현상이 일어난다. => Wh가 1보다 크면 지수적으로 증가하고 1보다 작으면 지수적으로 감소하게 된다. \n",
        "Wh가 행렬이라면 행렬의 특잇값이 척도가 되는데 특잇값은 데이터가 얼마나 퍼져있는지를 나타내게 된다. 특잇값의 최대값이 1보다 큰지 여부를 따지면 기울기 크기가 어떻게 변할지 예측할 수 있다.\n",
        "\n",
        "기울기 폭발의 대책 : 기울기 클리핑(전통적인 기법) : threshold를 넘으면 기울기를 작게 조정\n",
        "\n",
        "기울기 소실과 LSTM\n",
        "\n",
        "LSTM은 추가로 c라는 기억 셀의 추가 경로가 존재한다. \n",
        "h_t = tanh(c_t)\n",
        "\n",
        "게이트는 보통 시그모이드 함수를 사용한다."
      ]
    }
  ]
}