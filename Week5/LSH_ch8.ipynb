{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSH_ch7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6xRqE4gi10U"
      },
      "source": [
        "어텐션\n",
        "\n",
        "seq2seq의 문제점 \n",
        "\n",
        "encoder의 문제점 : 어떠한 길이의 문장을 넣어도 무조건 고정 길이의 벡터(hidden_state)가 output으로 나옴. => 정보의 유실이 불가피 함.\n",
        "\n",
        "encoder의 개선 : 각 시간별 모든 은닉상태를 이용\n",
        "\n",
        "Decoder의 개선 1(weight sum)\n",
        "\n",
        "단어의 대응관계 **alignmnet**\n",
        "\n",
        "신경망으로 하고 싶은 것 : 단어들의 alignment추출 => 각 시각에서 decoder에 입력된 단어와 대응관계인 단어의 벡터를 hidden_state에서 골라내겠다는 의미\n",
        "\n",
        "문제점 : 선택하는 과정은 미분할 수 없음.\n",
        "\n",
        "해결법 : 모든 단어를 선택하되 가중치를 두어 계산함 => 미분가능 해짐.=> output : 맥락벡터\n",
        "\n",
        "Decoder의 개선 2(attention weight)\n",
        "\n",
        "가중치를 구하는 과정\n",
        "\n",
        "hs와 h의 내적 => 두 벡터가 얼마나 같은 방향을 향하고 있는가(유사도)\n",
        "\n",
        "내적이후 softmax => 각 단어에 대해서 정규화된 점수를 가짐(가중치)\n",
        "\n",
        "Decoder의 개선 3\n",
        "\n",
        "affine 계층은 attention에서 출력된 맥락벡터와 해당 시간의 은닉계층을 concat해서 받게 된다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At_qQGzW51T8"
      },
      "source": [
        "양방향 RNN\n",
        "\n",
        "인코더에서 한 단어에 대응하는 벡터에 시간 상 앞에 있는 단어들의 정보가 인코딩되어 들어간다. 그래서 주변, 즉 뒤에 있는 단어의 정보도 균형있게 담고 싶을 때에 양방향RNN을 사용하면 된다.\n",
        "\n",
        "2개의 은닉상태들을 연결할지 합할지 평균낼지 어떠한 방법들을 사용할지는 임의로 선택하면 된다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXT7wmIdMF__"
      },
      "source": [
        "attention의 사용방법\n",
        "\n",
        "앞에서는 affine계층이 맥락벡터를 concat해서 받았지만 \n",
        "\n",
        "다른 방법으로는 다음 t+1 의 LSTM계층이 맥락벡터를 입력받음. 무엇이 더 좋은 방법인지는 결과가 나오기전까지는 알 수 없음.\n",
        "\n",
        "RNN의 층을 깊게 할 시 사용되는 기법 : skip connection(residual connection, shoert-cut)\n",
        "\n",
        "시간방향 기울기 소실 또는 폭발의 대응법 : gradient clipping, LSTM gate\n",
        "\n",
        "깊이 방향 기울기 소실 대비법 : skip connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trx955UkPelv"
      },
      "source": [
        "트랜스포머\n",
        "\n",
        "셀프 어텐션이용\n",
        "\n",
        "어텐션계층의 input이 encoder의 hs와 decoder의 hs가 아닌 같은 hs를 집어넣어 어텐션을 시킨다. \n",
        "\n",
        "병렬계산을 할 수 있음.\n",
        "\n",
        "어텐션은 내부 단어들끼리의 대응관계를 좀 더 잘 표현해줌."
      ]
    }
  ]
}