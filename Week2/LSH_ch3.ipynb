{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt5EU-A7ZfrU"
      },
      "source": [
        "Ch.3 word2vec\n",
        "\n",
        "이 장에서는 '추론 기반 기법'을 살펴 본다. \n",
        "단순한 word2vec을 구현하여 이해하고자 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgl7A-EnaS72"
      },
      "source": [
        "통계 기반의 문제점 => 거대 행렬에 SVD기법을 적용하는 것은 현실적이지 않다.(비용 O(n^3))\n",
        "\n",
        "반면 추론 기반의 기법은 미니 배치 학습을 사용할 수 있기 때문에 병렬계산도 가능해서 학습 속도가 빠름."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r1VukR4b7t2"
      },
      "source": [
        "추론 기반 기법이란?\n",
        "\n",
        "주변 단어(맥락)가 주어졌을 때 ?의 단어를 추측하는 작업\n",
        "\n",
        "input : 주변단어\n",
        "output : 추측 단어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYqmBwjnee2m"
      },
      "source": [
        "신경망에서의 단어 처리\n",
        "\n",
        "단어 => 고정 길이의 벡터(원핫 벡터)\n",
        "\n",
        "벡터 변환 방법\n",
        "\n",
        "1. 벡터의 차원 : 총 어휘의 수 \n",
        "\n",
        "2. if 인덱스 == 단어 ID : 1\n",
        "else : 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5IsiqO_mQRf",
        "outputId": "30eb68d9-3ca6-456e-8f67-7426a09b2f7e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "W = np.random.randn(7, 3)\n",
        "h = np.matmul(c, W)\n",
        "print(h)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.32624958 -1.58554966  0.90270204]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpdzJD3YlTcv"
      },
      "source": [
        "위 코드는 벡터를 원 핫벡터로 변환한 뒤 fully-connected layer를 이용하여 다시 변환 시켜주는 코드임.\n",
        "\n",
        "c의 차원 수는 2 (미니배치를 고려한 것)\n",
        "\n",
        "c와 W의 행렬곱은 c가 원핫벡터이므로 가중치의 행벡터를 하나 추출한 것과 같다.\n",
        "\n",
        "1장에서 구현한 MatMul계층에서 구현하려면 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq30hq2CZ3kY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "ab9f9c55-4171-49d2-bd33-b97e0de0cc3c"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import MatMul\n",
        "\n",
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "W = np.random.randn(7, 3)\n",
        "layer = MatMul(W)\n",
        "h = layer.forward(c)\n",
        "print(h)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4fcbd5eb9811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatMul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MsqhqbIqLZ4"
      },
      "source": [
        "단순한 word2vec => CBOW모델\n",
        "\n",
        "목적 : 추론을 활용하여 단어의 분산표현을 얻는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhZFFgCC2kAF"
      },
      "source": [
        "입력층이 여러개이면 입력층에 의해 변환된 값을 '평균'을 구해서 변환 시킴.\n",
        "\n",
        "ex)h_1와 h_2가 입력층에 의해 변환 되었다면 (h_1+h_2)/2를 구해서 출력층으로 보내주면 된다.\n",
        "\n",
        "입력층의 가중치가 바로 단어으 분산표현의 핵심이다.\n",
        "\n",
        "은닉층의 뉴런수는 입력층보다 적게 해야 밀집벡터가 완성됨.=> 인코딩\n",
        "\n",
        "출력층에서는 각 단어의 '점수'를 출력함. 여기에 softmax를 적용하면 확률을 얻을 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQHJd2cj2pJn",
        "outputId": "2ff7d55c-b22a-45f3-c98d-77cc62f1d582"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
        "\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np.random.randn(3, 7)\n",
        "\n",
        "#W_in을 공유함에 주의\n",
        "h_1 = np.matmul(c0, W_in)\n",
        "h_2 = np.matmul(c1, W_in)\n",
        "h = 0.5 * (h_1 + h_2)\n",
        "s = np.matmul(h, W_out)\n",
        "\n",
        "print(s)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.12881663  0.79031782 -0.58606968  0.10820439  1.43804941  0.83428569\n",
            "  -2.45277561]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iPeiJ6-FjLr"
      },
      "source": [
        "결과적으로 W_in과 W_out 모두에 단어의 출현벡터를 파악한 벡터가 학습됨.\n",
        "\n",
        "softmax with loss 계층을 사용하여 학습을 진행\n",
        "\n",
        "W_in의 각 행이 각 단어의 분산 표현에 해당된다면 W_out의 각 열은 단어의 분산표현이라고 할 수 있다.\n",
        "\n",
        "W_in만을 분산표현으로 사용하는 것이 일반적이지만 Glove에서는 두 가중치를 더해서 사용하는 것이 좋은 결과를 얻음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q8ptLt-JBKb"
      },
      "source": [
        "def preprocess(text) :\n",
        "  text= text.lower()\n",
        "  text = text.replace('.',' .')\n",
        "  words = text.split(' ')\n",
        "  \n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  for word in words :\n",
        "    if word not in word_to_id :\n",
        "      new_id = len(word_to_id)\n",
        "      word_to_id[word] = new_id\n",
        "      id_to_word[new_id] = word\n",
        "  \n",
        "  corpus = np.array([word_to_id[w] for w in words])\n",
        "  \n",
        "  return corpus, word_to_id, id_to_word"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPNHzTu3JU5v",
        "outputId": "5a680db9-d568-44c8-d739-11679afab3d3"
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "print(corpus)\n",
        "\n",
        "print(id_to_word)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUZywRJGKNJ8"
      },
      "source": [
        "def create_contexts_target(corpus, window_size=1):\n",
        "  target = corpus[window_size:-window_size]\n",
        "  contexts = []\n",
        "\n",
        "  for idx in range(window_size, len(corpus)-window_size):\n",
        "    cs = []\n",
        "    for t in range(-window_size, window_size + 1):\n",
        "      if t == 0:\n",
        "        continue\n",
        "      cs.append(corpus[idx + t])\n",
        "    contexts.append(cs)\n",
        "\n",
        "  return np.array(contexts), np.array(target)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SavGbczxPvkr",
        "outputId": "4d8669c2-15b4-4510-e824-aa43144a2a6b"
      },
      "source": [
        "contexts, target = create_contexts_target(corpus, window_size=1)\n",
        "\n",
        "print(contexts)\n",
        "\n",
        "print(target)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2]\n",
            " [1 3]\n",
            " [2 4]\n",
            " [3 1]\n",
            " [4 5]\n",
            " [1 6]]\n",
            "[1 2 3 4 1 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGXgh_-sSsDH"
      },
      "source": [
        "def convert_one_hot(corpus, vocab_size):\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDrHmZvMSuUp",
        "outputId": "8fc82d53-c3e9-436e-90e3-98c2062d110b"
      },
      "source": [
        "vocab_size = len(word_to_id)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "print(target)\n",
        "print(contexts)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0]]\n",
            "[[[1 0 0 0 0 0 0]\n",
            "  [0 0 1 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0 0 0]\n",
            "  [0 0 0 1 0 0 0]]\n",
            "\n",
            " [[0 0 1 0 0 0 0]\n",
            "  [0 0 0 0 1 0 0]]\n",
            "\n",
            " [[0 0 0 1 0 0 0]\n",
            "  [0 1 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 1 0 0]\n",
            "  [0 0 0 0 0 1 0]]\n",
            "\n",
            " [[0 1 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 1]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQDfc3n0Wa77"
      },
      "source": [
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.matmul(x, W)\n",
        "        self.x = x\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.matmul(dout, W.T)\n",
        "        dW = np.matmul(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6urkLL-qc7eE"
      },
      "source": [
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        x /= x.sum(axis=1, keepdims=True)\n",
        "    elif x.ndim == 1:\n",
        "        x = x - np.max(x)\n",
        "        x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnKJ4O38dM9I"
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # Convert to correct label index if input data is one-hot-vector\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    \n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T10e4wM2dQAu"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "\n",
        "        if self.t.size == self.y.size:\n",
        "            self.t = self.t.argmax(axis=1)\n",
        "\n",
        "        loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = self.y.copy()\n",
        "        dx[np.arange(batch_size), self.t] -= 1\n",
        "        dx *= dout\n",
        "        dx = dx / batch_size\n",
        "\n",
        "        return dx"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6RlZQ48yy7o"
      },
      "source": [
        "def remove_duplicate(params, grads):\n",
        "    params, grads = params[:], grads[:]\n",
        "\n",
        "    while True:\n",
        "        find_flg = False\n",
        "        L = len(params)\n",
        "\n",
        "        for i in range(0, L - 1):\n",
        "            for j in range(i + 1, L):\n",
        "                if params[i] is params[j]:\n",
        "                    grads[i] += grads[j]\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
        "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
        "                    grads[i] += grads[j].T\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "\n",
        "                if find_flg: break\n",
        "            if find_flg: break\n",
        "\n",
        "        if not find_flg: break\n",
        "\n",
        "    return params, grads"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtXQrJluy4hd"
      },
      "source": [
        "import time\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_list = []\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "        data_size = len(x)\n",
        "        max_iters = data_size // batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            idx = np.random.permutation(np.arange(data_size))\n",
        "            x = x[idx]\n",
        "            t = t[idx]\n",
        "\n",
        "            for iters in range(max_iters):\n",
        "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "                # Finding gradients and updating parameters\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)\n",
        "                \n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    avg_loss = total_loss / loss_count\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
        "                    self.loss_list.append(float(avg_loss))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = np.arange(len(self.loss_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.loss_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTbG5hcdzIm4"
      },
      "source": [
        "class Adam:\n",
        "    '''\n",
        "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
        "    '''\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = [], []\n",
        "            for param in params:\n",
        "                self.m.append(np.zeros_like(param))\n",
        "                self.v.append(np.zeros_like(param))\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "            \n",
        "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGynakHYdUZP"
      },
      "source": [
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # weight initialization\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # the preparation of a layer\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # List all weights and gradients\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # Set word distribution expression in member variables\n",
        "        self.word_vecs = W_in\n",
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "        return None"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bOwG1ZvxzPvY",
        "outputId": "fa7e15a8-bdab-4d39-bd26-5769b58d24e0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "plt.show()\n",
        "\n",
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id_to_word.items():\n",
        "    print(word, word_vecs[word_id])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 2 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 3 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 4 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 5 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 6 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 7 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 8 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 9 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 10 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
            "| epoch 11 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 12 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 13 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 14 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 15 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 16 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 17 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 18 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 19 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 20 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 21 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 22 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 23 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 24 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 25 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 26 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 27 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
            "| epoch 28 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 29 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 30 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 31 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 32 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 33 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 34 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 35 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
            "| epoch 36 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 37 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 38 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 39 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 40 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
            "| epoch 41 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 42 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 43 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 44 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
            "| epoch 45 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 46 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 47 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 48 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 49 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 50 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
            "| epoch 51 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 52 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 53 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
            "| epoch 54 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 55 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 56 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 57 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
            "| epoch 58 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 59 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
            "| epoch 60 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 61 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 62 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 63 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 64 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
            "| epoch 65 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 66 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
            "| epoch 67 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 68 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 69 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
            "| epoch 70 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 71 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
            "| epoch 72 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 73 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 74 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
            "| epoch 75 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 76 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
            "| epoch 77 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 78 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 79 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
            "| epoch 80 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 81 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 82 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
            "| epoch 83 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
            "| epoch 84 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 85 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 86 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
            "| epoch 87 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
            "| epoch 88 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
            "| epoch 89 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 90 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
            "| epoch 91 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 92 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
            "| epoch 93 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
            "| epoch 94 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
            "| epoch 95 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 96 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
            "| epoch 97 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
            "| epoch 98 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
            "| epoch 99 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
            "| epoch 100 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
            "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
            "| epoch 102 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 103 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
            "| epoch 104 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 105 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
            "| epoch 106 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 107 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
            "| epoch 108 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
            "| epoch 109 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 110 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
            "| epoch 111 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 112 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 113 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
            "| epoch 114 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
            "| epoch 115 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 116 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
            "| epoch 117 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
            "| epoch 118 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 119 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
            "| epoch 120 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 121 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
            "| epoch 122 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
            "| epoch 123 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
            "| epoch 124 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
            "| epoch 125 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
            "| epoch 126 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 127 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
            "| epoch 128 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 129 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
            "| epoch 130 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
            "| epoch 131 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
            "| epoch 132 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 133 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
            "| epoch 134 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
            "| epoch 135 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
            "| epoch 136 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
            "| epoch 137 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
            "| epoch 138 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
            "| epoch 139 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
            "| epoch 140 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 141 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
            "| epoch 142 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
            "| epoch 143 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 144 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
            "| epoch 145 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
            "| epoch 146 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
            "| epoch 147 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 148 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 149 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 150 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
            "| epoch 151 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 152 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
            "| epoch 153 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
            "| epoch 154 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
            "| epoch 155 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
            "| epoch 156 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
            "| epoch 157 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 158 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 159 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
            "| epoch 160 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 161 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
            "| epoch 162 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
            "| epoch 163 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
            "| epoch 164 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
            "| epoch 165 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 166 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 167 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
            "| epoch 168 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
            "| epoch 169 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
            "| epoch 170 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
            "| epoch 171 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 172 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 173 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 174 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
            "| epoch 175 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
            "| epoch 176 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
            "| epoch 177 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
            "| epoch 178 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 179 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 180 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 181 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
            "| epoch 182 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
            "| epoch 183 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 184 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 185 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 186 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 187 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 188 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 189 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 190 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
            "| epoch 191 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 192 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 193 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 194 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
            "| epoch 195 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
            "| epoch 196 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 197 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 198 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 199 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
            "| epoch 200 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 202 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 203 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 204 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 205 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 206 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 207 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
            "| epoch 208 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 209 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 210 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 211 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 212 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
            "| epoch 213 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 214 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 215 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 216 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 217 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 218 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 219 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 220 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 221 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 222 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 223 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 224 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 225 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
            "| epoch 226 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 227 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 228 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 229 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 230 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 231 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 232 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 233 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
            "| epoch 234 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 235 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 236 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 237 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 238 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 239 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 240 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 241 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 242 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 243 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 244 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 245 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 246 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 247 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 248 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 249 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
            "| epoch 250 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 251 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 252 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 253 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 254 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
            "| epoch 255 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 256 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 257 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 258 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 259 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 260 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 261 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 262 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 263 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 264 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 265 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 266 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 267 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 268 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 269 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
            "| epoch 270 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 271 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 272 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 273 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 274 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 275 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 276 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 277 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 278 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 279 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 280 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 281 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 282 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 283 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 284 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 285 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 286 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 287 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 288 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 289 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
            "| epoch 290 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 291 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 292 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 293 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 294 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 295 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 296 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 297 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 298 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 299 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 300 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 302 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 303 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 304 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 305 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 306 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 307 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 308 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 309 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 310 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 311 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 312 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
            "| epoch 313 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 314 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 315 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 316 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 317 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 318 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 319 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 320 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 321 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 322 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 323 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 324 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 325 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 326 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 327 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 328 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 329 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 330 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 331 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
            "| epoch 332 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 333 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 334 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 335 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 336 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 337 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 338 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
            "| epoch 339 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 340 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 341 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
            "| epoch 342 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 343 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 344 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 345 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 346 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 347 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
            "| epoch 348 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 349 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 350 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 351 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 352 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 353 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 354 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 355 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 356 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 357 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
            "| epoch 358 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 359 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 360 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 361 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 362 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 363 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 364 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 365 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 366 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 367 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 368 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
            "| epoch 369 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 370 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 371 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 372 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
            "| epoch 373 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 374 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 375 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 376 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 377 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 378 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 379 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 380 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 381 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 382 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 383 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
            "| epoch 384 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 385 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 386 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 387 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 388 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 389 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
            "| epoch 390 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 391 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 392 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 393 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 394 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 395 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 396 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 397 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 398 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 399 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 400 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 402 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 403 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 404 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 405 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 406 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 407 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 408 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 409 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 410 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 411 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 412 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 413 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
            "| epoch 414 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 415 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 416 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 417 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 418 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 419 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 420 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 421 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 422 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 423 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 424 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 425 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 426 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 427 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 428 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 429 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 430 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 431 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 432 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 433 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 434 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 435 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 436 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 437 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 438 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 439 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 440 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 441 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 442 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 443 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 444 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 445 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 446 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 447 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 448 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 449 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 450 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 451 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 452 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
            "| epoch 453 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 454 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 455 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 456 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 457 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 458 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 459 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 460 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 461 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 462 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 463 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 464 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 465 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
            "| epoch 466 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 467 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 468 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 469 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 470 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 471 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 472 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 473 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 474 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 475 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 476 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 477 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 478 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 479 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 480 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 481 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 482 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
            "| epoch 483 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 484 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 485 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 486 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 487 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 488 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 489 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 490 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 491 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 492 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 493 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
            "| epoch 494 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 495 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
            "| epoch 496 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 497 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 498 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
            "| epoch 499 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 500 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 502 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 503 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 504 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 505 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 506 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 507 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 508 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 509 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 510 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 511 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 512 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 513 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 514 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 515 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 516 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 517 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 518 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 519 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 520 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
            "| epoch 521 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 522 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 523 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 524 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 525 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 526 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 527 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 528 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 529 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 530 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 531 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 532 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 533 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 534 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 535 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 536 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 537 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 538 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 539 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 540 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 541 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
            "| epoch 542 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 543 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 544 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 545 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 546 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 547 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 548 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 549 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 550 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
            "| epoch 551 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 552 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 553 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 554 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 555 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 556 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 557 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 558 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 559 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 560 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 561 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 562 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 563 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 564 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
            "| epoch 565 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 566 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 567 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 568 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 569 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 570 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 571 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 572 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 573 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 574 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 575 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 576 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 577 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
            "| epoch 578 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 579 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 580 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 581 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 582 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 583 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 584 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 585 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 586 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 587 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 588 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 589 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 590 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 591 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 592 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
            "| epoch 593 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 594 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 595 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 596 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 597 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 598 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 599 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 600 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 602 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 603 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 604 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 605 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 606 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 607 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 608 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 609 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 610 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
            "| epoch 611 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 612 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 613 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 614 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
            "| epoch 615 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 616 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 617 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 618 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 619 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 620 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 621 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 622 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 623 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 624 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
            "| epoch 625 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 626 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 627 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 628 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 629 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 630 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 631 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 632 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 633 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
            "| epoch 634 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 635 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
            "| epoch 636 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 637 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 638 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 639 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 640 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 641 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 642 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 643 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 644 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 645 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 646 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 647 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 648 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 649 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 650 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 651 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 652 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 653 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 654 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 655 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 656 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 657 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 658 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 659 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 660 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 661 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 662 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 663 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 664 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 665 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 666 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 667 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 668 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 669 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 670 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 671 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 672 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 673 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 674 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 675 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 676 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 677 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 678 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 679 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 680 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
            "| epoch 681 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 682 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 683 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 684 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 685 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 686 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 687 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
            "| epoch 688 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 689 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 690 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 691 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 692 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 693 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 694 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 695 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 696 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 697 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
            "| epoch 698 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 699 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 700 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 702 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 703 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 704 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 705 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
            "| epoch 706 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 707 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 708 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 709 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 710 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 711 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 712 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 713 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 714 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 715 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 716 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 717 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 718 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 719 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 720 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 721 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 722 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 723 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 724 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 725 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 726 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 727 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 728 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 729 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 730 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 731 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 732 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 733 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 734 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 735 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
            "| epoch 736 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 737 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
            "| epoch 738 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 739 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 740 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 741 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 742 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 743 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 744 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 745 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 746 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 747 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 748 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 749 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 750 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 751 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 752 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 753 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 754 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 755 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 756 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 757 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
            "| epoch 758 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 759 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
            "| epoch 760 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 761 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 762 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 763 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
            "| epoch 764 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 765 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 766 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 767 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
            "| epoch 768 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 769 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 770 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 771 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 772 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 773 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 774 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 775 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 776 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 777 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 778 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 779 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 780 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 781 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 782 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
            "| epoch 783 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 784 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 785 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 786 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 787 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 788 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 789 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
            "| epoch 790 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 791 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 792 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 793 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 794 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 795 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 796 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 797 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 798 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 799 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 800 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
            "| epoch 802 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 803 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 804 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 805 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 806 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 807 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 808 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
            "| epoch 809 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 810 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 811 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 812 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
            "| epoch 813 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 814 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
            "| epoch 815 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 816 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
            "| epoch 817 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 818 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 819 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 820 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
            "| epoch 821 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 822 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 823 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 824 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 825 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
            "| epoch 826 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
            "| epoch 827 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
            "| epoch 828 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 829 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 830 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 831 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 832 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 833 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 834 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 835 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 836 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
            "| epoch 837 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
            "| epoch 838 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 839 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 840 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 841 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 842 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
            "| epoch 843 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 844 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 845 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 846 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
            "| epoch 847 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 848 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 849 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
            "| epoch 850 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 851 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 852 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 853 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 854 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 855 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 856 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 857 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 858 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 859 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 860 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 861 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 862 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
            "| epoch 863 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
            "| epoch 864 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
            "| epoch 865 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 866 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 867 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
            "| epoch 868 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
            "| epoch 869 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 870 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 871 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 872 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 873 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 874 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 875 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 876 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
            "| epoch 877 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
            "| epoch 878 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 879 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 880 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 881 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 882 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
            "| epoch 883 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
            "| epoch 884 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
            "| epoch 885 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
            "| epoch 886 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
            "| epoch 887 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
            "| epoch 888 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 889 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 890 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 891 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
            "| epoch 892 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 893 |  iter 1 / 2 | time 1[s] | loss 1.04\n",
            "| epoch 894 |  iter 1 / 2 | time 1[s] | loss 0.62\n",
            "| epoch 895 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 896 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 897 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 898 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
            "| epoch 899 |  iter 1 / 2 | time 1[s] | loss 1.02\n",
            "| epoch 900 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 901 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
            "| epoch 902 |  iter 1 / 2 | time 1[s] | loss 0.74\n",
            "| epoch 903 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
            "| epoch 904 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 905 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
            "| epoch 906 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 907 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
            "| epoch 908 |  iter 1 / 2 | time 1[s] | loss 1.11\n",
            "| epoch 909 |  iter 1 / 2 | time 1[s] | loss 0.62\n",
            "| epoch 910 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
            "| epoch 911 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 912 |  iter 1 / 2 | time 1[s] | loss 0.90\n",
            "| epoch 913 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 914 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 915 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 916 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 917 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
            "| epoch 918 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
            "| epoch 919 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
            "| epoch 920 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 921 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 922 |  iter 1 / 2 | time 1[s] | loss 1.14\n",
            "| epoch 923 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 924 |  iter 1 / 2 | time 1[s] | loss 0.76\n",
            "| epoch 925 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 926 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
            "| epoch 927 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 928 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 929 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
            "| epoch 930 |  iter 1 / 2 | time 1[s] | loss 0.61\n",
            "| epoch 931 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 932 |  iter 1 / 2 | time 1[s] | loss 0.92\n",
            "| epoch 933 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
            "| epoch 934 |  iter 1 / 2 | time 1[s] | loss 0.92\n",
            "| epoch 935 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 936 |  iter 1 / 2 | time 1[s] | loss 1.14\n",
            "| epoch 937 |  iter 1 / 2 | time 1[s] | loss 0.59\n",
            "| epoch 938 |  iter 1 / 2 | time 1[s] | loss 1.15\n",
            "| epoch 939 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 940 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
            "| epoch 941 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 942 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 943 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 944 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
            "| epoch 945 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 946 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 947 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 948 |  iter 1 / 2 | time 1[s] | loss 0.96\n",
            "| epoch 949 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 950 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 951 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 952 |  iter 1 / 2 | time 1[s] | loss 1.13\n",
            "| epoch 953 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 954 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 955 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 956 |  iter 1 / 2 | time 1[s] | loss 1.15\n",
            "| epoch 957 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 958 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 959 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
            "| epoch 960 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 961 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 962 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 963 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 964 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 965 |  iter 1 / 2 | time 1[s] | loss 1.15\n",
            "| epoch 966 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 967 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
            "| epoch 968 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 969 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
            "| epoch 970 |  iter 1 / 2 | time 1[s] | loss 0.73\n",
            "| epoch 971 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 972 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 973 |  iter 1 / 2 | time 1[s] | loss 1.01\n",
            "| epoch 974 |  iter 1 / 2 | time 1[s] | loss 0.70\n",
            "| epoch 975 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 976 |  iter 1 / 2 | time 1[s] | loss 0.75\n",
            "| epoch 977 |  iter 1 / 2 | time 1[s] | loss 1.00\n",
            "| epoch 978 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 979 |  iter 1 / 2 | time 1[s] | loss 0.85\n",
            "| epoch 980 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 981 |  iter 1 / 2 | time 1[s] | loss 0.74\n",
            "| epoch 982 |  iter 1 / 2 | time 1[s] | loss 0.97\n",
            "| epoch 983 |  iter 1 / 2 | time 1[s] | loss 0.87\n",
            "| epoch 984 |  iter 1 / 2 | time 1[s] | loss 0.88\n",
            "| epoch 985 |  iter 1 / 2 | time 1[s] | loss 0.83\n",
            "| epoch 986 |  iter 1 / 2 | time 1[s] | loss 0.72\n",
            "| epoch 987 |  iter 1 / 2 | time 1[s] | loss 1.02\n",
            "| epoch 988 |  iter 1 / 2 | time 1[s] | loss 0.69\n",
            "| epoch 989 |  iter 1 / 2 | time 1[s] | loss 1.14\n",
            "| epoch 990 |  iter 1 / 2 | time 1[s] | loss 0.69\n",
            "| epoch 991 |  iter 1 / 2 | time 1[s] | loss 0.74\n",
            "| epoch 992 |  iter 1 / 2 | time 1[s] | loss 0.86\n",
            "| epoch 993 |  iter 1 / 2 | time 1[s] | loss 1.13\n",
            "| epoch 994 |  iter 1 / 2 | time 1[s] | loss 0.71\n",
            "| epoch 995 |  iter 1 / 2 | time 1[s] | loss 0.89\n",
            "| epoch 996 |  iter 1 / 2 | time 1[s] | loss 0.98\n",
            "| epoch 997 |  iter 1 / 2 | time 1[s] | loss 0.59\n",
            "| epoch 998 |  iter 1 / 2 | time 1[s] | loss 0.99\n",
            "| epoch 999 |  iter 1 / 2 | time 1[s] | loss 0.84\n",
            "| epoch 1000 |  iter 1 / 2 | time 1[s] | loss 0.86\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "you [ 1.1697924  1.1884756 -1.1436954 -1.1503748  1.1866586]\n",
            "say [-1.1892817 -1.1902381  1.1228154  1.198114  -1.1660086]\n",
            "goodbye [ 0.74630207  0.7778211  -0.81404203 -0.78010535  0.6750232 ]\n",
            "and [-0.99551904 -0.9412282   1.2830328   1.0362003  -1.3110992 ]\n",
            "i [ 0.7853035   0.7885493  -0.8151438  -0.7644631   0.63009936]\n",
            "hello [ 1.1550705  1.2062888 -1.164045  -1.1648403  1.2154895]\n",
            ". [-1.0717089  -1.1197091   0.30512652  0.9829607  -0.30143386]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1w28DoKwNVo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKLbbl07l0VN"
      },
      "source": [
        "3.5.1 CBOW모델과 확률\n",
        "\n",
        "L=-logP(w_t|w_(t-1),w_(t+1))\n",
        "\n",
        "음의 로그 가능도 => 교차 크로스 엔트로피\n",
        "\n",
        "skip-gram의 경우 L값을 구할 때 동시발생 확률이므로 확률을 곱하지만 로그를 씌워주면 더하기로 변해서 2개의 단어의 조건부확률을 더해주는 식으로 변한다. \n",
        "\n",
        "말뭉치가 커질 수록 skip-gram모델이 더 뛰어난 경향이 있다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d_JrL0yI-0"
      },
      "source": [
        "word2vec는 가중치를 다시 학습 시킬 수 있어서 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수핼할 수 있음."
      ]
    }
  ]
}